{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNYr+R04a5v/dIbANmWdd6L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kkarbasi/FaMiniGPT/blob/master/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from faminigpt.utils import clone_persian_poems_colab\n",
        "from faminigpt.gpt_modules import Head, MultiHeadAttention, MLP, PoetryModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "cf8L-etIxIzd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = clone_persian_poems_colab('hafez')"
      ],
      "metadata": {
        "id": "MvMV6IAmUZcq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[0:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKgRGFda45jP",
        "outputId": "54c1f033-d79f-4ece-84d8-06ae45ac8d3c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  \t\n",
            "الا یا ایها الساقی ادر کاسا و ناولها\n",
            "که عشق آسان نمود اول ولی افتاد مشکل ها\n",
            "به بوی نافه ای کاخر صبا زان طره بگشاید\n",
            "ز تاب جعد مشکینش چه خون افتاد در دل ها\n",
            "مرا در منزل جانان چه امن عیش چون هر دم\n",
            "جرس فریاد می دارد که بربندید محمل ها\n",
            "به می سجاده رنگین کن گرت پیر مغان گوید\n",
            "که سالک بی خبر نبود ز راه و رسم منزل ها\n",
            "شب تاریک و بیم موج و گردابی چنین هایل\n",
            "کجا دانند حال ما سبکباران ساحل ها\n",
            "همه کارم ز خود کامی به بدنامی کشید آخر\n",
            "نهان کی ماند آن رازی کز او سازند محفل ها\n",
            "حضوری گر همی خواهی از او غایب مشو \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple encoder and decoders\n",
        "chars = list(set(text))\n",
        "vocab_size = len(chars)\n",
        "stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "encode = lambda x: [stoi[ch] for ch in x]\n",
        "decode = lambda x: ''.join([itos[i] for i in x])"
      ],
      "metadata": {
        "id": "3b1wZSfwDRj5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gykZJRLCECHh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train = data[:n]\n",
        "test = data[n:]\n",
        "print(train.shape, test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMotI6gEDkrg",
        "outputId": "b0a394db-fafc-45b4-8ea7-77a2558bcda1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([275735]) torch.Size([30638])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1331)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# model\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "embedding_size = 64\n",
        "multihead_size = 64\n",
        "num_heads = 4\n",
        "num_transformers = 4\n",
        "dropout = 0.00\n",
        "\n",
        "assert multihead_size % num_heads == 0\n",
        "head_size = int(multihead_size/num_heads)\n",
        "\n",
        "\n",
        "# training loop\n",
        "lr=1e-3\n",
        "eval_iters = 200\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n"
      ],
      "metadata": {
        "id": "EVgvqULAD16T"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train if split == 'train' else test\n",
        "  idx = torch.randint(0, len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in idx])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in idx])\n",
        "  x = x.to(device)\n",
        "  y = y.to(device)\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "NdlISd1qEl70"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = get_batch('train')\n",
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYxN0Tn1D57k",
        "outputId": "dc91c0b0-d100-40dc-d1bf-71f12b38cbf2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 32])\n",
            "torch.Size([16, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test_head():\n",
        "  x, _ = get_batch('train')\n",
        "  B, T = x.shape\n",
        "  model = Head(head_size, embedding_size, block_size, dropout)\n",
        "  model = model.to(device)\n",
        "  embedding = nn.Embedding(vocab_size, embedding_size, device=device)\n",
        "  head_output = model(embedding(x))\n",
        "  assert head_output.shape == (B, T, head_size)\n",
        "test_head()\n"
      ],
      "metadata": {
        "id": "XZh6Qkh2UjgV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_multihead():\n",
        "  x, _ = get_batch('train')\n",
        "  B, T = x.shape\n",
        "  model = MultiHeadAttention(num_heads, head_size, embedding_size, block_size, dropout)\n",
        "  model = model.to(device)\n",
        "  embedding = nn.Embedding(vocab_size, embedding_size, device=device)\n",
        "  multihead_output = model(embedding(x))\n",
        "  assert multihead_output.shape == (B, T, head_size*num_heads)\n",
        "test_multihead()"
      ],
      "metadata": {
        "id": "7eukgVWaX-Zm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test_poetry_model():\n",
        "  x, y = get_batch('train')\n",
        "  B, T = x.shape\n",
        "  model = PoetryModel(vocab_size,\n",
        "                     num_heads,\n",
        "                     head_size,\n",
        "                     embedding_size,\n",
        "                     multihead_size,\n",
        "                     block_size,\n",
        "                     num_transformers,\n",
        "                     dropout,\n",
        "                     device)\n",
        "  model = model.to(device)\n",
        "\n",
        "  # without y (for generation)\n",
        "  model_out = model(x)\n",
        "  assert model_out[0].shape == (B, T, vocab_size)\n",
        "\n",
        "  # with y (for training)\n",
        "  model_out = model(x, y)\n",
        "  assert model_out[0].shape == (B*T, vocab_size)\n",
        "\n",
        "test_poetry_model()"
      ],
      "metadata": {
        "id": "GecJmc2SWH9B"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PoetryModel(vocab_size,\n",
        "                     num_heads,\n",
        "                     head_size,\n",
        "                     embedding_size,\n",
        "                     multihead_size,\n",
        "                     block_size,\n",
        "                     num_transformers,\n",
        "                     dropout,\n",
        "                     device)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "HSstsWvnhA5s"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "-ArwOJzfEP9p"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'test']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "13JkLUuHGbjh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for step in range(max_iters):\n",
        "  if step % eval_interval == 0:\n",
        "    eval_loss = estimate_loss()\n",
        "    print(f\"At step {step} train loss is {eval_loss['train']} test loss is {eval_loss['test']}\")\n",
        "  xb, yb = get_batch('train')\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYX_DsCZEoGd",
        "outputId": "9012e7c0-7a9f-4ea7-8d06-1b15212e7fcc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At step 0 train loss is 3.84089732170105 test loss is 3.8649797439575195\n",
            "At step 100 train loss is 2.6457202434539795 test loss is 2.7931811809539795\n",
            "At step 200 train loss is 2.5704538822174072 test loss is 2.7577590942382812\n",
            "At step 300 train loss is 2.5173888206481934 test loss is 2.729361057281494\n",
            "At step 400 train loss is 2.477102279663086 test loss is 2.70937442779541\n",
            "At step 500 train loss is 2.4502675533294678 test loss is 2.676898241043091\n",
            "At step 600 train loss is 2.4188530445098877 test loss is 2.6389105319976807\n",
            "At step 700 train loss is 2.3975510597229004 test loss is 2.625314235687256\n",
            "At step 800 train loss is 2.368927240371704 test loss is 2.6086440086364746\n",
            "At step 900 train loss is 2.339495897293091 test loss is 2.5896494388580322\n",
            "At step 1000 train loss is 2.3310859203338623 test loss is 2.6080329418182373\n",
            "At step 1100 train loss is 2.3045074939727783 test loss is 2.56663179397583\n",
            "At step 1200 train loss is 2.290968656539917 test loss is 2.5552797317504883\n",
            "At step 1300 train loss is 2.278513193130493 test loss is 2.560452938079834\n",
            "At step 1400 train loss is 2.2781591415405273 test loss is 2.5497450828552246\n",
            "At step 1500 train loss is 2.2648465633392334 test loss is 2.5480215549468994\n",
            "At step 1600 train loss is 2.2450411319732666 test loss is 2.528249979019165\n",
            "At step 1700 train loss is 2.233889102935791 test loss is 2.519592761993408\n",
            "At step 1800 train loss is 2.237382650375366 test loss is 2.5224432945251465\n",
            "At step 1900 train loss is 2.2055160999298096 test loss is 2.501854181289673\n",
            "At step 2000 train loss is 2.2091195583343506 test loss is 2.514338731765747\n",
            "At step 2100 train loss is 2.1941914558410645 test loss is 2.502203941345215\n",
            "At step 2200 train loss is 2.199723243713379 test loss is 2.485696315765381\n",
            "At step 2300 train loss is 2.1909334659576416 test loss is 2.495835781097412\n",
            "At step 2400 train loss is 2.1854231357574463 test loss is 2.48986554145813\n",
            "At step 2500 train loss is 2.1683857440948486 test loss is 2.483402967453003\n",
            "At step 2600 train loss is 2.172206163406372 test loss is 2.4902539253234863\n",
            "At step 2700 train loss is 2.1569032669067383 test loss is 2.47933030128479\n",
            "At step 2800 train loss is 2.1604151725769043 test loss is 2.470987319946289\n",
            "At step 2900 train loss is 2.146238327026367 test loss is 2.4692795276641846\n",
            "At step 3000 train loss is 2.141540288925171 test loss is 2.4703774452209473\n",
            "At step 3100 train loss is 2.141120195388794 test loss is 2.4602320194244385\n",
            "At step 3200 train loss is 2.1385953426361084 test loss is 2.4581329822540283\n",
            "At step 3300 train loss is 2.129666805267334 test loss is 2.44537353515625\n",
            "At step 3400 train loss is 2.114520311355591 test loss is 2.439862012863159\n",
            "At step 3500 train loss is 2.10981822013855 test loss is 2.444690465927124\n",
            "At step 3600 train loss is 2.115039825439453 test loss is 2.440441370010376\n",
            "At step 3700 train loss is 2.1196157932281494 test loss is 2.4434876441955566\n",
            "At step 3800 train loss is 2.099397897720337 test loss is 2.430190324783325\n",
            "At step 3900 train loss is 2.0933752059936523 test loss is 2.4450674057006836\n",
            "At step 4000 train loss is 2.097527503967285 test loss is 2.4440059661865234\n",
            "At step 4100 train loss is 2.097672700881958 test loss is 2.454653263092041\n",
            "At step 4200 train loss is 2.108084201812744 test loss is 2.4328155517578125\n",
            "At step 4300 train loss is 2.0824809074401855 test loss is 2.4235246181488037\n",
            "At step 4400 train loss is 2.085008144378662 test loss is 2.412125587463379\n",
            "At step 4500 train loss is 2.065032958984375 test loss is 2.4019668102264404\n",
            "At step 4600 train loss is 2.066915273666382 test loss is 2.410919189453125\n",
            "At step 4700 train loss is 2.0670619010925293 test loss is 2.414776086807251\n",
            "At step 4800 train loss is 2.055485248565674 test loss is 2.413823127746582\n",
            "At step 4900 train loss is 2.0573225021362305 test loss is 2.4239869117736816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1, 1), dtype=torch.long, device=device) + stoi['\\n']\n",
        "print(decode(model.generate(idx, 2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6_Kjc__hYMJ",
        "outputId": "5171acd3-23f6-4a53-d6e9-cf1fb6cd8eb6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "بادگور از و شکو بار ای پیامی به چابید مسکل\n",
            "با برفت کند کید بودی را همین بخود کدیر\n",
            "او صبا باندو جام ابرود کای فحران\n",
            "چو چمن چه راه خاکه و درا پرو داونی\n",
            "به چو هر نیزی و خلب که عارصون دارم\n",
            "تا چینید دهدی روران جای که عوکبی\n",
            "چمحلخو چشمه اس جلاده تواننی\n",
            "گشه گشه و زکر ز جامی سعر ملاصور بتازان بیاد\n",
            "کین صحمی العش چه عود جهانم\n",
            "و خبا جهف منکله کوچون فراه نفاد\n",
            "گو رسوه پرده نهح ز باریم خنگی به تو کند\n",
            "چه دست امود در چو من گشوید گران نمیدی\n",
            "مگر حامعر بیماری رنخشان ذول چن پی\n",
            "که دادار گشه حکایم زین\n",
            "اختم می رباز بودیش سر که بیند خدم نکن\n",
            "دنوشید ندید روی لبام که جامینا طلب\n",
            "آشقهز بردش خدا ار که دورشیست به آنکن و نظوستی\n",
            "به رود ره که تا ببگا آیید گو روی\n",
            "چاه نویان دشی کو وجای اندان است\n",
            "گو گوی ولامت و به نما بی همهی مجان هزار من\n",
            "چو نقد اهر این و مدای دهانه قاک برافاز\n",
            "به بیشماور به در کین منزان نمی کند\n",
            "حدیی تروارانی که آید سیره خاک خونی\n",
            "پرده وقران دهر می گوشه ما همیم\n",
            "که گعندد در غبا پیمام دل دل از لدود\n",
            "را رفوشفه جامرت\n",
            "چو ترو به در از تو نیمه سخن ز عشقاله پیرد\n",
            "خون سربش امن ضلعت سخن بکرد\n",
            "کندی زبوحی نوله سروز چندان سیخت\n",
            "غلف که از این کان درگانم همتم نمید\n",
            "افاشکنه به درم ای نمی عمر گل افتیدی\n",
            "ز شد نروخه با عکب نیستی کردم صالهن ای چنداناز\n",
            "که نقاله نظه بر سیماده آن آن آن مخوابیت گونان\n",
            "چه گر تا ناز کار من پیمرولانه\n",
            "چو بره همن توک فقیب عیرنان نکنی\n",
            "چه سرا این چمن از دلا توقت خشم\n",
            "طار خون بگاه آن داری به کیند\n",
            "ایمی که نمامده از دو خون جال مشاق قول\n",
            "کردم پردم مشکو که مندانمی\n",
            "از بگذر ماه ز می کمختی کهر از اگر مسب\n",
            "باشد گل قبه شماد که کامش با نگوشم سر چیدم\n",
            "هفا نعضحی تو کاشم نگابه زندهانی هنر زلن اسبلرو ساگی\n",
            "می و جهان نید عارام کردار و کنم\n",
            "آی که به کو ما خوان در تو ز نرانیم\n",
            "نهر یک ایی از تعن نگفت دشص است عشق است\n",
            "جز اندات گی بابود ما که حکایم کشید چمون می لطف ندارد\n",
            "با افتیان و باشر می این چین\n",
            "حافظاس چه انز کناختی سر خون دم\n",
            "بوی دوشم تو با چه جار حسنبان هننی\n",
            "گوی میا ز به گفتمت ایمن تو چه گدان بید\n",
            "صبود مگر روتانی جسی نبرد\n",
            "آن پر چه به صحم به منی عهاریم کند\n",
            "بوی رجاب حافظ دست که گرخنگی است نهیست\n",
            "در فروش مغه بگریند یاری چنده دوس گه شمی\n",
            "چلن به پرده شار داش دلآ خدا مشکد\n",
            "گوی که هجا بازن از حرام زدییان\n",
            "موه بادان کا فر بآیید ا\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.tensor(encode('کاوه'), dtype=torch.long, device=device).unsqueeze(dim=0)\n",
        "print(decode(model.generate(idx, 600)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCHSUwvnI8ov",
        "outputId": "b511bee7-5ee5-4943-a4a2-b2edabd4115e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "کاوه کند\n",
            "بی در هر آمده که خیر ای جمیان حافظ\n",
            "گز مرغ دله دم می بسیند که می باشی نیدم\n",
            "از دیار دو ای نوک ای تنتمابه غلب\n",
            "هوای ببود نفرون من به تخور از اندوسان بازآید\n",
            "و به به آن و فر نیم خود جود مدوشم\n",
            "بود می خاک آن دویدی همان مهرا تطف\n",
            "حکیف او لمحک بر کهند کرده تو کند\n",
            "تیخ مقای مقد شکن آهیم کنم\n",
            "به ز و رخم دست از باید مایمارونی\n",
            "بیار نماند صبا به ما برود آرده خور\n",
            "خیرین رغ و نماندحش به دوش\n",
            "ناله معند چو بیدام گریید دیند خوش باد\n",
            "شمعری غمی آن گفتمم شدم حسن\n",
            "مصند آنمانه کشی شهریب تو می بود مبو بینن\n",
            "بکنشی خوشم صاراحتمی می نایبم و را\n",
            "آن ز اهر که ننیمار با تطریب خلوی\n",
            "محره گو نزم را لبا روی هندان لطفه گرانید\n",
            "مگر ز چن\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "814XCqkPXDUr"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}